###############################################################
# 完整环境与训练配置
# 目标：参数明确、可验证、物理意义清晰
# 说明：包含环境参数、SAC算法参数、训练控制参数
###############################################################

# 1) SUMO 场景文件相对路径
# 范围：字符串（相对项目根目录），必须存在
sumo_cfg_relpath: "sumo/grid1x3.sumocfg"

# 2) 速度离散集合（km/h）
# 范围：整数列表，长度≥3，单调递增；建议取值在 [20, 120]
discrete_speeds_kph: [30, 35, 40, 45, 50]

# 3) 决策间隔（秒）
# 范围：正整数；物理意义：每次限速更新的时间间隔
# 建议：与信号周期/绿波 offset 匹配（常见 15~20s）
# 当前配置：17s 匹配绿波 offset（J0→J1: 17s, J0→J2: 34s）
# 信号周期：170s = 17s × 10（已修正为17s的整数倍，确保绿波协调）
decision_interval: 17

# 4) 仿真总时长（秒）
# 范围：正整数；物理意义：单回合仿真的最大秒数
max_sim_seconds: 3600

# 5) VSL 控制的东向三段（车道 ID 列表）
# 范围：各段为字符串列表；物理意义：对这些车道上的车辆施加限速控制
lane_groups:
  upstream: ["W_J0_1"]
  mid: ["J0_J1_1"]
  down: ["J1_J2_1"]

# 6) VSL 控制的西向三段（车道 ID 列表）
# 范围：各段为字符串列表；可为空；与东向一致的三段结构
lane_groups_wb:
  upstream: ["E_J2_1"]
  mid: ["J2_J1_1"]
  down: ["J1_J0_1"]

# 7) 信号灯 ID 列表
# 范围：字符串列表；物理意义：用于观测与指标统计
tls_ids: ["J0", "J1", "J2"]

# 8) CAV 渗透率（比例）
# 范围：[0.0, 1.0]；物理意义：车队中 CAV 占比，用于生成 rou.xml
penetration: 0.3

# 9) 奖励权重：速度效率
# 范围：非负浮点；物理意义：鼓励更高的平均速度
reward_speed_weight: 3.5

# 10) 奖励权重：拥堵惩罚
# 范围：非负浮点；物理意义：惩罚停车/排队，提高通行效率
reward_congestion_weight: 0.8

# ============================================================
# 环境可选参数（有默认值）
# ============================================================

# 11) 仿真预热步数（秒）
# 范围：非负整数；物理意义：环境reset后预热仿真的步数，让车辆进入稳态
warmup_steps: 5

# 12) 动作变化惩罚系数
# 范围：非负浮点；物理意义：惩罚相邻决策步限速变化过大，鼓励平滑控制
reward_change_penalty: 0.1

# 13) 单次限速变化上限（km/h）
# 范围：正浮点；物理意义：相邻决策步之间单段限速变化的最大值（硬约束）
max_delta_kph: 10.0

# 14) 奖励裁剪范围（可选）
# 范围：浮点数或null；物理意义：裁剪奖励到指定范围，提高训练稳定性
# reward_clip_min: null
# reward_clip_max: null

# 15) SUMO后端选择
# 范围："auto" | "libsumo" | "traci"；推荐auto（优先libsumo，失败回退traci）
backend: "auto"

# 16) CAV类型ID
# 范围：字符串；物理意义：SUMO中CAV车辆的类型名称，用于识别被控车辆
cav_type_id: "CAV"

# ============================================================
# SAC 算法参数
# ============================================================

# 17) 训练回合数
# 范围：正整数；物理意义：训练总回合数（可被命令行--episodes覆盖）
episodes: 200

# 18) 折扣因子
# 范围：(0, 1)；建议：[0.99, 0.999]；物理意义：未来奖励的折扣率
# 与decision_interval耦合：interval越大，gamma应越大
gamma: 0.995

# 19) 学习率（通用）
# 范围：正浮点；建议：[1e-5, 1e-3]；同时作为actor和critic的默认学习率
learning_rate: 3e-4

# 20) Actor网络学习率
# 范围：正浮点；建议：通常≤learning_rate；若不设置则使用learning_rate
actor_lr: 2e-4

# 21) Critic网络学习率
# 范围：正浮点；建议：通常≥learning_rate；若不设置则使用learning_rate  
critic_lr: 3e-4

# 22) Polyak软更新系数
# 范围：(0, 1)；建议：[0.001, 0.01]；物理意义：目标网络更新速度
sac_tau: 0.005

# 23) 熵温度系数
# 范围：正浮点；建议：[0.05, 0.5]；物理意义：探索vs利用权衡
sac_alpha: 0.15

# 24) 隐藏层大小
# 范围：正整数；建议：[128, 512]；物理意义：神经网络隐藏层神经元数量
hidden_size: 256

# 25) 梯度裁剪范数
# 范围：正浮点；建议：[1.0, 10.0]；物理意义：防止梯度爆炸
grad_clip_norm: 5.0

# 26) 目标网络更新间隔
# 范围：正整数；建议：1（每步更新）；物理意义：多少次训练步后更新目标网络
sac_target_update_interval: 1

# ============================================================
# 经验回放与训练控制
# ============================================================

# 27) 经验回放缓冲区容量
# 范围：正整数；建议：[100000, 1000000]；物理意义：存储的最大经验数
buffer_size: 200000

# 28) 批次大小
# 范围：正整数；建议：[32, 256]；物理意义：每次更新采样的经验数量
batch_size: 128

# 29) 预填充步数
# 范围：非负整数；建议：[1000, 10000]或0；物理意义：训练前用随机策略收集的步数
prefill_steps: 5000

# 30) 周期性检查点保存间隔
# 范围：正整数或0；物理意义：每N回合保存一次检查点，0表示关闭
save_every_n_episodes: 50

# 31) 设备选择
# 范围："auto" | "cuda" | "cpu"；推荐auto（有GPU用GPU，否则CPU）
device: "auto"

# 32) Baseline固定速度（评估用）
# 范围：浮点；单位km/h；物理意义：评估baseline策略时使用的固定限速值
# 建议设为discrete_speeds_kph的最大值
baseline_fixed_speed_kph: 50.0

# ============================================================
# 参数关系说明
# ============================================================
# 1. decision_interval 与 gamma 耦合：
#    - interval=5s → gamma≈0.99
#    - interval=17s → gamma≈0.995
#    - interval=30s → gamma≈0.999
#
# 2. learning_rate 与 batch_size 耦合：
#    - batch_size越大，learning_rate可以适当增大
#    - 建议actor_lr ≤ critic_lr（策略更新更保守）
#
# 3. buffer_size 与 episodes 耦合：
#    - episodes越多，buffer可以越大
#    - 建议buffer_size ≈ episodes * max_decision_steps
#
# 4. prefill_steps 建议：
#    - 至少 ≥ batch_size
#    - 通常设为 batch_size * 10~100